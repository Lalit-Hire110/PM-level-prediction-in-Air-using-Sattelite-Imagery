{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ff6780-81ac-4146-bb90-171c20c6e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROFESSIONAL PM2.5 MODEL TESTING SUITE ===\n",
      "Loading all artifacts and performing comprehensive model validation...\n",
      "\n",
      "1. ARTIFACT VERIFICATION\n",
      "✓ Found: model_xgb_prod_phase6.joblib\n",
      "✓ Found: features_phase2_locked.json\n",
      "✓ Found: model_metadata_phase6.json\n",
      "✓ Found: dataset_phase2_leakage_safe.parquet\n",
      "\n",
      "2. MODEL LOADING\n",
      "✓ Model loaded from: model_xgb_prod_phase6.joblib\n",
      "  Model type: XGBRegressor\n",
      "✓ Feature list loaded: 72 features\n",
      "✓ Metadata loaded\n",
      "\n",
      "3. DATASET PREPARATION\n",
      "✓ Dataset loaded: (102720, 167)\n",
      "✓ All 72 features available\n",
      "✓ Final dataset ready: X(102720, 72), y(102720,)\n",
      "\n",
      "4. MODEL VALIDATION TESTS\n",
      "\n",
      "4.1 Basic Prediction Test\n",
      "✓ Prediction test passed\n",
      "  Sample predictions: [ 92.26  97.2  102.77 180.54 191.54]\n",
      "  Prediction range: 39.1 - 244.9 μg/m³\n",
      "\n",
      "4.2 TimeSeriesSplit Cross-Validation\n",
      "✓ TimeSeriesSplit Results:\n",
      "   fold    rmse     mae     r2  n_samples\n",
      "0     1  32.160  18.077  0.868      17120\n",
      "1     2  33.560  18.183  0.724      17120\n",
      "2     3  21.843   6.616  0.578      17120\n",
      "3     4  29.433   8.281  0.470      17120\n",
      "4     5  22.908  12.139  0.737      17120\n",
      "  Mean RMSE: 27.98 ± 5.34\n",
      "  Mean R²: 0.676 ± 0.154\n",
      "\n",
      "4.3 Leave-One-Station-Out Test\n",
      "✓ LOSO Results:\n",
      "   station_fold    rmse     r2  n_samples\n",
      "0             1  33.543  0.816      20470\n",
      "1             2  35.023  0.771      20659\n",
      "2             3  28.780  0.827      20746\n",
      "3             4  21.270  0.710      20137\n",
      "4             5  26.673  0.827      20708\n",
      "  Mean RMSE: 29.06 ± 5.53\n",
      "  Mean R²: 0.790 ± 0.050\n",
      "\n",
      "4.4 Feature Importance Analysis\n",
      "✓ Top 10 Feature Importances:\n",
      "  PM2.5_lag3: 0.3891\n",
      "  lag3_x_stability: 0.1157\n",
      "  pm25_lag6: 0.1030\n",
      "  PM2.5_lag24: 0.0405\n",
      "  PM25_past_rolling6_std: 0.0248\n",
      "  hour_cos: 0.0190\n",
      "  inversion_strength: 0.0136\n",
      "  PM25_past_rolling12_std: 0.0136\n",
      "  stability: 0.0127\n",
      "  tp_rolling24: 0.0126\n",
      "\n",
      "5. FINAL VALIDATION SUMMARY\n",
      "==================================================\n",
      "Model Type: XGBRegressor\n",
      "Features: 72\n",
      "Training Data: 102,720 samples\n",
      "Stations: 20\n",
      "Target Transform: log1p\n",
      "Sample RMSE: 45.64 μg/m³\n",
      "CV RMSE: 27.98 ± 5.34\n",
      "CV R²: 0.676 ± 0.154\n",
      "LOSO RMSE: 29.06 ± 5.53\n",
      "Spatial Generalization: Good\n",
      "\n",
      "✓ MODEL IS READY FOR DEPLOYMENT\n",
      "\n",
      "=== TESTING COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "# ROBUST MODEL TESTING SCRIPT - Uses all saved artifacts with error handling\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== PROFESSIONAL PM2.5 MODEL TESTING SUITE ===\")\n",
    "print(\"Loading all artifacts and performing comprehensive model validation...\")\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "USE_LOG_TARGET = True\n",
    "\n",
    "def inv_if_log(a):\n",
    "    \"\"\"Inverse log1p transform if model was trained on log scale\"\"\"\n",
    "    return np.expm1(a) if USE_LOG_TARGET else a\n",
    "\n",
    "# Step 1: Verify and load all required artifacts\n",
    "print(\"\\n1. ARTIFACT VERIFICATION\")\n",
    "required_files = {\n",
    "    \"model\": \"model_xgb_prod_phase6.joblib\",\n",
    "    \"features\": \"features_phase2_locked.json\", \n",
    "    \"metadata\": \"model_metadata_phase6.json\",\n",
    "    \"dataset\": \"dataset_phase2_leakage_safe.parquet\"\n",
    "}\n",
    "\n",
    "# Check if files exist, with fallbacks\n",
    "for name, filename in required_files.items():\n",
    "    if not os.path.exists(filename):\n",
    "        if name == \"model\":\n",
    "            # Try alternative model names\n",
    "            alternatives = [\"model_xgb_phase4_final.joblib\", \"pm25_model_production.joblib\"]\n",
    "            for alt in alternatives:\n",
    "                if os.path.exists(alt):\n",
    "                    required_files[\"model\"] = alt\n",
    "                    print(f\"✓ Using alternative model: {alt}\")\n",
    "                    break\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No model file found. Tried: {filename}, {alternatives}\")\n",
    "        elif name == \"dataset\":\n",
    "            # Try Phase 1 dataset and rebuild features\n",
    "            alt = \"dataset_feature1_phase1_clean.parquet\"\n",
    "            if os.path.exists(alt):\n",
    "                required_files[\"dataset\"] = alt\n",
    "                print(f\"✓ Using Phase 1 dataset: {alt} (will rebuild AR features)\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No dataset found. Tried: {filename}, {alt}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Required file not found: {filename}\")\n",
    "    else:\n",
    "        print(f\"✓ Found: {filename}\")\n",
    "\n",
    "# Step 2: Load model and metadata\n",
    "print(\"\\n2. MODEL LOADING\")\n",
    "try:\n",
    "    model = joblib.load(required_files[\"model\"])\n",
    "    print(f\"✓ Model loaded from: {required_files['model']}\")\n",
    "    print(f\"  Model type: {type(model).__name__}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load model: {e}\")\n",
    "\n",
    "# Load feature list\n",
    "try:\n",
    "    with open(required_files[\"features\"], \"r\") as f:\n",
    "        feature_data = json.load(f)\n",
    "    features = feature_data[\"features\"] if isinstance(feature_data, dict) else feature_data\n",
    "    print(f\"✓ Feature list loaded: {len(features)} features\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load features: {e}\")\n",
    "\n",
    "# Load metadata if available\n",
    "metadata = {}\n",
    "try:\n",
    "    with open(required_files[\"metadata\"], \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"✓ Metadata loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Metadata not available: {e}\")\n",
    "\n",
    "# Step 3: Load and prepare dataset\n",
    "print(\"\\n3. DATASET PREPARATION\")\n",
    "try:\n",
    "    df = pd.read_parquet(required_files[\"dataset\"])\n",
    "    print(f\"✓ Dataset loaded: {df.shape}\")\n",
    "    \n",
    "    # If using Phase 1 dataset, rebuild AR features\n",
    "    if \"phase1\" in required_files[\"dataset\"]:\n",
    "        print(\"  Rebuilding leakage-safe AR features...\")\n",
    "        \n",
    "        # Ensure numeric station_quality\n",
    "        if \"station_quality\" in df.columns and pd.api.types.is_categorical_dtype(df[\"station_quality\"]):\n",
    "            df[\"station_quality\"] = df[\"station_quality\"].cat.codes\n",
    "        \n",
    "        # Rebuild AR features\n",
    "        df = df.sort_values([\"station_id\", \"timestamp_utc\"]).copy()\n",
    "        g = df.groupby(\"station_id\")[\"PM2.5\"]\n",
    "        \n",
    "        # Lags\n",
    "        for h in [1,3,6,12,24,48,72]:\n",
    "            col = f\"pm25_lag{h}\"\n",
    "            if col not in df.columns:\n",
    "                df[col] = g.shift(h)\n",
    "        \n",
    "        # Past-only rollings\n",
    "        s = g.shift(1)  # Exclude current t\n",
    "        for w in [3,6,12,24]:\n",
    "            mean_col = f\"PM25_past_rolling{w}_mean\"\n",
    "            std_col = f\"PM25_past_rolling{w}_std\"\n",
    "            if mean_col not in df.columns:\n",
    "                df[mean_col] = s.rolling(w, min_periods=max(1,w//2)).mean()\n",
    "            if std_col not in df.columns:\n",
    "                df[std_col] = s.rolling(w, min_periods=max(2,w//2)).std()\n",
    "        \n",
    "        # Interactions (only if base features exist)\n",
    "        if all(c in df.columns for c in [\"pm25_lag3\", \"stability\"]):\n",
    "            df[\"lag3_x_stability\"] = df[\"pm25_lag3\"] * df[\"stability\"]\n",
    "        \n",
    "        if \"tp\" in df.columns and \"tp_rolling24\" not in df.columns:\n",
    "            s_tp = df.groupby(\"station_id\")[\"tp\"].shift(1)\n",
    "            df[\"tp_rolling24\"] = s_tp.rolling(24, min_periods=12).mean()\n",
    "        \n",
    "        if all(c in df.columns for c in [\"pm25_lag3\", \"tp_rolling24\"]):\n",
    "            df[\"lag3_x_tp24\"] = df[\"pm25_lag3\"] * df[\"tp_rolling24\"]\n",
    "        \n",
    "        print(f\"  AR features rebuilt. New shape: {df.shape}\")\n",
    "    \n",
    "    # Create target variable\n",
    "    if USE_LOG_TARGET and \"PM25_log1p\" not in df.columns:\n",
    "        df[\"PM25_log1p\"] = np.log1p(df[\"PM2.5\"])\n",
    "    \n",
    "    # Verify features exist and filter to available ones\n",
    "    available_features = [f for f in features if f in df.columns]\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"⚠ Missing {len(missing_features)} features: {missing_features[:5]}...\")\n",
    "        print(f\"  Using {len(available_features)} available features\")\n",
    "        features = available_features\n",
    "    else:\n",
    "        print(f\"✓ All {len(features)} features available\")\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df[features].copy()\n",
    "    y = df[\"PM25_log1p\"] if USE_LOG_TARGET else df[\"PM2.5\"]\n",
    "    \n",
    "    # Clean any remaining issues\n",
    "    for col in X.columns:\n",
    "        if pd.api.types.is_categorical_dtype(X[col]):\n",
    "            X[col] = X[col].cat.codes\n",
    "        elif X[col].dtype == 'object':\n",
    "            try:\n",
    "                X[col] = pd.to_numeric(X[col])\n",
    "            except:\n",
    "                X[col] = pd.factorize(X[col])[0]\n",
    "    \n",
    "    print(f\"✓ Final dataset ready: X{X.shape}, y{y.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Dataset preparation failed: {e}\")\n",
    "\n",
    "# Step 4: Model validation tests\n",
    "print(\"\\n4. MODEL VALIDATION TESTS\")\n",
    "\n",
    "# Test 4.1: Basic prediction test\n",
    "print(\"\\n4.1 Basic Prediction Test\")\n",
    "try:\n",
    "    sample_X = X.head(100).fillna(X.mean())  # Handle any NaNs for testing\n",
    "    sample_pred = model.predict(sample_X)\n",
    "    sample_pred_orig = inv_if_log(sample_pred)\n",
    "    \n",
    "    print(f\"✓ Prediction test passed\")\n",
    "    print(f\"  Sample predictions: {sample_pred_orig[:5].round(2)}\")\n",
    "    print(f\"  Prediction range: {sample_pred_orig.min():.1f} - {sample_pred_orig.max():.1f} μg/m³\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Prediction test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test 4.2: TimeSeriesSplit Cross-Validation\n",
    "print(\"\\n4.2 TimeSeriesSplit Cross-Validation\")\n",
    "try:\n",
    "    tss = TimeSeriesSplit(n_splits=5, gap=3)\n",
    "    cv_results = []\n",
    "    \n",
    "    for fold, (tr, va) in enumerate(tss.split(X), 1):\n",
    "        # Handle NaNs in targets\n",
    "        tr_mask = ~y.iloc[tr].isna()\n",
    "        va_mask = ~y.iloc[va].isna()\n",
    "        \n",
    "        if tr_mask.sum() == 0 or va_mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        tr_idx = np.array(tr)[tr_mask]\n",
    "        va_idx = np.array(va)[va_mask]\n",
    "        \n",
    "        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        pred = model.predict(X_va)\n",
    "        y_true = inv_if_log(y_va)\n",
    "        y_pred = inv_if_log(pred)\n",
    "        \n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        cv_results.append({\n",
    "            \"fold\": fold,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2,\n",
    "            \"n_samples\": len(y_true)\n",
    "        })\n",
    "    \n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    print(\"✓ TimeSeriesSplit Results:\")\n",
    "    print(cv_df.round(3))\n",
    "    print(f\"  Mean RMSE: {cv_df['rmse'].mean():.2f} ± {cv_df['rmse'].std():.2f}\")\n",
    "    print(f\"  Mean R²: {cv_df['r2'].mean():.3f} ± {cv_df['r2'].std():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ TimeSeriesSplit test failed: {e}\")\n",
    "\n",
    "# Test 4.3: Leave-One-Station-Out (LOSO)\n",
    "print(\"\\n4.3 Leave-One-Station-Out Test\")\n",
    "try:\n",
    "    if \"station_id\" in df.columns:\n",
    "        groups = df[\"station_id\"].values\n",
    "        n_stations = len(np.unique(groups))\n",
    "        gkf = GroupKFold(n_splits=min(5, n_stations))\n",
    "        loso_results = []\n",
    "        \n",
    "        for fold, (tr, te) in enumerate(gkf.split(X, y, groups), 1):\n",
    "            X_te, y_te = X.iloc[te], y.iloc[te]\n",
    "            \n",
    "            pred = model.predict(X_te)\n",
    "            y_true = inv_if_log(y_te)\n",
    "            y_pred = inv_if_log(pred)\n",
    "            \n",
    "            rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            \n",
    "            loso_results.append({\n",
    "                \"station_fold\": fold,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"n_samples\": len(y_true)\n",
    "            })\n",
    "        \n",
    "        loso_df = pd.DataFrame(loso_results)\n",
    "        print(\"✓ LOSO Results:\")\n",
    "        print(loso_df.round(3))\n",
    "        print(f\"  Mean RMSE: {loso_df['rmse'].mean():.2f} ± {loso_df['rmse'].std():.2f}\")\n",
    "        print(f\"  Mean R²: {loso_df['r2'].mean():.3f} ± {loso_df['r2'].std():.3f}\")\n",
    "    else:\n",
    "        print(\"⚠ No station_id column found, skipping LOSO test\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ LOSO test failed: {e}\")\n",
    "\n",
    "# Test 4.4: Feature importance verification\n",
    "print(\"\\n4.4 Feature Importance Analysis\")\n",
    "try:\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = pd.Series(model.feature_importances_, index=features)\n",
    "        top_features = importances.nlargest(10)\n",
    "        print(\"✓ Top 10 Feature Importances:\")\n",
    "        for feat, imp in top_features.items():\n",
    "            print(f\"  {feat}: {imp:.4f}\")\n",
    "    else:\n",
    "        print(\"⚠ Model does not have feature_importances_ attribute\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Feature importance test failed: {e}\")\n",
    "\n",
    "# Step 5: Summary Report\n",
    "print(\"\\n5. FINAL VALIDATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Overall statistics\n",
    "    last_pred = model.predict(X.sample(min(1000, len(X)), random_state=SEED))\n",
    "    last_true = y.sample(min(1000, len(y)), random_state=SEED)\n",
    "    overall_rmse = sqrt(mean_squared_error(inv_if_log(last_true), inv_if_log(last_pred)))\n",
    "    \n",
    "    print(f\"Model Type: {type(model).__name__}\")\n",
    "    print(f\"Features: {len(features)}\")\n",
    "    print(f\"Training Data: {len(X):,} samples\")\n",
    "    print(f\"Stations: {df['station_id'].nunique() if 'station_id' in df.columns else 'Unknown'}\")\n",
    "    print(f\"Target Transform: {'log1p' if USE_LOG_TARGET else 'none'}\")\n",
    "    print(f\"Sample RMSE: {overall_rmse:.2f} μg/m³\")\n",
    "    \n",
    "    if 'cv_results' in locals():\n",
    "        print(f\"CV RMSE: {cv_df['rmse'].mean():.2f} ± {cv_df['rmse'].std():.2f}\")\n",
    "        print(f\"CV R²: {cv_df['r2'].mean():.3f} ± {cv_df['r2'].std():.3f}\")\n",
    "    \n",
    "    if 'loso_results' in locals():\n",
    "        print(f\"LOSO RMSE: {loso_df['rmse'].mean():.2f} ± {loso_df['rmse'].std():.2f}\")\n",
    "        print(f\"Spatial Generalization: {'Good' if loso_df['rmse'].mean() < 40 else 'Moderate'}\")\n",
    "    \n",
    "    print(\"\\n✓ MODEL IS READY FOR DEPLOYMENT\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Summary generation failed: {e}\")\n",
    "\n",
    "print(\"\\n=== TESTING COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c0b7d60-7531-4ce9-92a2-d1b02f735c2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3d32194-f02e-4f5e-82ec-f090f28628b2",
   "metadata": {},
   "source": [
    "## Unit Test for Lag/Rolling Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96533fe3-53e7-4a2e-bf73-b98b651613ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def test_shift_then_roll_no_leakage():\n",
    "    # Create synthetic data\n",
    "    timestamps = pd.date_range(\"2025-01-01\", periods=10, freq=\"H\")\n",
    "    df = pd.DataFrame({\n",
    "        \"station_id\": [\"S1\"] * 10,\n",
    "        \"timestamp_utc\": timestamps,\n",
    "        \"PM2.5\": np.arange(10, 20, dtype=float)\n",
    "    })\n",
    "\n",
    "    # Build lag1 and roll3_mean incorrectly\n",
    "    df[\"bad_roll3\"] = df.groupby(\"station_id\")[\"PM2.5\"].rolling(3).mean().reset_index(0, drop=True)\n",
    "    # Build correctly: shift before rolling\n",
    "    grp = df.groupby(\"station_id\")[\"PM2.5\"]\n",
    "    df[\"good_roll3\"] = grp.shift(1).rolling(3).mean().reset_index(0, drop=True)\n",
    "\n",
    "    # At index 2 (third hour), bad_roll3 includes PM2.5 at t=2 => should equal mean(10,11,12) = 11\n",
    "    assert df.loc[2, \"bad_roll3\"] == np.mean([10,11,12])\n",
    "    # good_roll3 at index 2 uses values at t=0,1 => mean(10,11) = 10.5\n",
    "    assert np.isclose(df.loc[2, \"good_roll3\"], 10.5)\n",
    "    \n",
    "    # Check that no good_roll3 value equals the current PM2.5\n",
    "    leaks = (df[\"good_roll3\"] == df[\"PM2.5\"])\n",
    "    assert not leaks.any(), \"Leakage detected: good_roll3 should never match current PM2.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31a218-9714-4f21-a9bf-281d4c1409f7",
   "metadata": {},
   "source": [
    "## Nested CV Wrapper for Feature Selection + Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658a1993-dfee-4b3b-a152-91187e262ad5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 RMSE: 37.0762\n",
      "Fold 2 RMSE: 42.2544\n",
      "Fold 3 RMSE: 24.6084\n",
      "Fold 4 RMSE: 34.1179\n",
      "Fold 5 RMSE: 29.8785\n",
      "Nested CV RMSEs: [np.float64(37.076218566915074), np.float64(42.254445007820486), np.float64(24.608426791022765), np.float64(34.11793025207396), np.float64(29.878480708095157)]\n",
      "Mean RMSE: 33.58710026518549\n"
     ]
    }
   ],
   "source": [
    "# Full fixed script: Corr+VIF selector + nested TSS CV + XGBoost early stopping (robust)\n",
    "import json\n",
    "import inspect\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Corr+VIF selector (unchanged logic, with safety guards)\n",
    "# -----------------------------\n",
    "class CorrVIFSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, corr_thresh=0.85, vif_thresh=10.0, protected=None):\n",
    "        self.corr_thresh = corr_thresh\n",
    "        self.vif_thresh = vif_thresh\n",
    "        self.protected = set(protected or [])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy().astype(float, errors=\"ignore\")\n",
    "\n",
    "        # correlation clustering\n",
    "        corr = df.corr().abs()\n",
    "        to_drop = set()\n",
    "        cols = list(corr.columns)\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(i+1, len(cols)):\n",
    "                a, b = cols[i], cols[j]\n",
    "                if corr.loc[a, b] > self.corr_thresh:\n",
    "                    # drop the one with smaller mean-correlation\n",
    "                    drop = b if corr[a].mean() >= corr[b].mean() else a\n",
    "                    to_drop.add(drop)\n",
    "        feats = [f for f in df.columns if f not in to_drop]\n",
    "\n",
    "        # VIF pruning with safety for small sets\n",
    "        def compute_vif(df_sel):\n",
    "            # return dict {col: vif}\n",
    "            if df_sel.shape[1] <= 1:\n",
    "                return {col: 1.0 for col in df_sel.columns}\n",
    "            Xc = df_sel.fillna(df_sel.median(numeric_only=True))\n",
    "            Xc = sm.add_constant(Xc)\n",
    "            vifs = {}\n",
    "            for i, col in enumerate(df_sel.columns):\n",
    "                # safety guard: if there's no i+1 column, skip\n",
    "                if i+1 >= Xc.shape[1]:\n",
    "                    vifs[col] = 1.0\n",
    "                    continue\n",
    "                ycol = Xc.iloc[:, i+1]\n",
    "                Xother = Xc.drop(Xc.columns[i+1], axis=1)\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    r2 = sm.OLS(ycol, Xother).fit().rsquared\n",
    "                vifs[col] = np.inf if (1 - r2) == 0 else 1.0 / (1.0 - r2)\n",
    "            return vifs\n",
    "\n",
    "        keep = feats.copy()\n",
    "        while True:\n",
    "            if len(keep) <= 1:\n",
    "                break\n",
    "            vifs = compute_vif(df[keep])\n",
    "            # pick worst VIF\n",
    "            worst, val = max(vifs.items(), key=lambda kv: kv[1])\n",
    "            if val <= self.vif_thresh:\n",
    "                break\n",
    "            if worst in self.protected:\n",
    "                # try to find next non-protected offender\n",
    "                offenders = [f for f, v in vifs.items() if (f not in self.protected) and (v > self.vif_thresh)]\n",
    "                if offenders:\n",
    "                    worst = offenders[0]\n",
    "                    val = vifs[worst]\n",
    "                else:\n",
    "                    # nothing safe to drop\n",
    "                    break\n",
    "            # drop worst\n",
    "            keep.remove(worst)\n",
    "        self.selected_features_ = keep\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.selected_features_].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Robust XGBoost fit helper\n",
    "# -----------------------------\n",
    "class BoosterWrapper:\n",
    "    \"\"\"Wrap xgboost.Booster to provide .predict like sklearn estimator\"\"\"\n",
    "    def __init__(self, booster):\n",
    "        self.booster = booster\n",
    "        self.best_iteration = getattr(booster, \"best_iteration\", None)\n",
    "    def predict(self, Xp):\n",
    "        # accept pandas DataFrame or numpy array\n",
    "        return self.booster.predict(xgb.DMatrix(Xp))\n",
    "\n",
    "def fit_xgb_with_es(params, X_tr, y_tr, X_va, y_va, early_rounds=50, verbose=False):\n",
    "    \"\"\"\n",
    "    Fit an XGBoost model with early stopping in a way that works across versions.\n",
    "    Returns either a fitted XGBRegressor or BoosterWrapper with .predict(X) semantics.\n",
    "    \"\"\"\n",
    "    # ensure copies to avoid side-effects\n",
    "    X_tr = X_tr.copy()\n",
    "    X_va = X_va.copy()\n",
    "    # encode categorical columns to integer codes (if any)\n",
    "    for c in X_tr.select_dtypes(include=[\"category\"]).columns:\n",
    "        X_tr[c] = X_tr[c].cat.codes\n",
    "    for c in X_va.select_dtypes(include=[\"category\"]).columns:\n",
    "        X_va[c] = X_va[c].cat.codes\n",
    "    for c in X_tr.select_dtypes(include=[\"object\"]).columns:\n",
    "        try:\n",
    "            X_tr[c] = pd.to_numeric(X_tr[c])\n",
    "        except Exception:\n",
    "            X_tr[c] = pd.factorize(X_tr[c])[0]\n",
    "    for c in X_va.select_dtypes(include=[\"object\"]).columns:\n",
    "        try:\n",
    "            X_va[c] = pd.to_numeric(X_va[c])\n",
    "        except Exception:\n",
    "            X_va[c] = pd.factorize(X_va[c])[0]\n",
    "\n",
    "    # Inspect sklearn wrapper fit signature\n",
    "    try:\n",
    "        fit_sig = inspect.signature(XGBRegressor.fit)\n",
    "        fit_params = set(fit_sig.parameters.keys())\n",
    "    except Exception:\n",
    "        fit_params = set()\n",
    "\n",
    "    # create sklearn wrapper\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # 1) Try callbacks API\n",
    "    if \"callbacks\" in fit_params:\n",
    "        try:\n",
    "            # try import EarlyStopping from xgboost.callback if available\n",
    "            try:\n",
    "                from xgboost.callback import EarlyStopping\n",
    "                es_cb = EarlyStopping(rounds=early_rounds, save_best=True, maximize=False)\n",
    "            except Exception:\n",
    "                es_cb = xgb.callback.EarlyStopping(rounds=early_rounds, save_best=True)\n",
    "            model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=verbose, callbacks=[es_cb])\n",
    "            return model\n",
    "        except TypeError:\n",
    "            warnings.warn(\"callbacks path failed: falling back\", RuntimeWarning)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"callbacks path raised {e}; falling back\", RuntimeWarning)\n",
    "\n",
    "    # 2) Try early_stopping_rounds param\n",
    "    if \"early_stopping_rounds\" in fit_params:\n",
    "        try:\n",
    "            model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=verbose, early_stopping_rounds=early_rounds)\n",
    "            return model\n",
    "        except TypeError:\n",
    "            warnings.warn(\"early_stopping_rounds path failed: falling back\", RuntimeWarning)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"early_stopping_rounds path raised {e}; falling back\", RuntimeWarning)\n",
    "\n",
    "    # 3) Fallback to xgb.train()\n",
    "    xgb_params = params.copy()\n",
    "    num_round = int(xgb_params.pop(\"n_estimators\", 1000))\n",
    "    # map sklearn-style keys\n",
    "    if \"random_state\" in xgb_params:\n",
    "        xgb_params[\"seed\"] = xgb_params.pop(\"random_state\")\n",
    "    if \"n_jobs\" in xgb_params:\n",
    "        xgb_params[\"nthread\"] = int(xgb_params.pop(\"n_jobs\"))\n",
    "    xgb_params.setdefault(\"objective\", \"reg:squarederror\")\n",
    "    xgb_params.setdefault(\"eval_metric\", \"rmse\")\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_va, label=y_va)\n",
    "    watchlist = [(dval, \"validation\")]\n",
    "    bst = xgb.train(xgb_params, dtrain, num_boost_round=num_round, evals=watchlist,\n",
    "                    early_stopping_rounds=early_rounds, verbose_eval=False)\n",
    "    return BoosterWrapper(bst)\n",
    "\n",
    "# -----------------------------\n",
    "# Main experiment (nested TSS CV)\n",
    "# -----------------------------\n",
    "# load dataset & locked features\n",
    "df = pd.read_parquet(\"dataset_phase2_leakage_safe.parquet\")\n",
    "with open(\"features_phase2_locked.json\") as f:\n",
    "    features = json.load(f)[\"features\"]\n",
    "\n",
    "# ensure feature presence (do not silently drop locked features)\n",
    "missing = [f for f in features if f not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Locked features missing from dataset: {missing}\\n\"\n",
    "                   \"You must run the feature-engineering pipeline that produced them.\")\n",
    "\n",
    "X = df[features].copy()\n",
    "y = np.log1p(df[\"PM2.5\"])\n",
    "\n",
    "# coerce object/category columns to numeric safely\n",
    "for c in X.columns:\n",
    "    if pd.api.types.is_categorical_dtype(X[c]):\n",
    "        X[c] = X[c].cat.codes\n",
    "    elif X[c].dtype == object:\n",
    "        try:\n",
    "            X[c] = pd.to_numeric(X[c])\n",
    "        except Exception:\n",
    "            X[c] = pd.factorize(X[c])[0]\n",
    "\n",
    "# example tuned params\n",
    "best_params = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits=5, gap=3)\n",
    "results = []\n",
    "\n",
    "for fold_i, (tr, va) in enumerate(tss.split(X), start=1):\n",
    "    X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "    y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "    # 1) Fit selector on training fold only\n",
    "    selector = CorrVIFSelector(protected=[\"PM2.5_lag3\", \"stability\"])\n",
    "    selector.fit(X_tr)\n",
    "    X_tr_sel = selector.transform(X_tr)\n",
    "    X_va_sel = selector.transform(X_va)\n",
    "\n",
    "    # 2) Train using robust helper that will adapt to your XGBoost\n",
    "    model = fit_xgb_with_es(best_params, X_tr_sel, y_tr, X_va_sel, y_va, early_rounds=50, verbose=False)\n",
    "\n",
    "    # 3) Evaluate\n",
    "    pred = model.predict(X_va_sel)\n",
    "    rmse = np.sqrt(mean_squared_error(np.expm1(y_va), np.expm1(pred)))\n",
    "    results.append(rmse)\n",
    "\n",
    "    print(f\"Fold {fold_i} RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"Nested CV RMSEs:\", results)\n",
    "print(\"Mean RMSE:\", np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301145a0-5cb3-489a-846b-4c5045492c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
